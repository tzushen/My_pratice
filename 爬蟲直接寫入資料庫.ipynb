{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 每日爬蟲直接寫入資料庫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import datetime\n",
    "import re\n",
    "import time\n",
    "import json\n",
    "import ast  #轉換成json需要套件\n",
    "import string\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient()\n",
    "database = client[\"test1\"] \n",
    "collection = database[\"ltn05\"] \n",
    "\n",
    "\n",
    "today = datetime.date.today()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "yesterday1 = yesterday.strftime(\"%Y%m%d\")  \n",
    "#print yesterday1\n",
    "\n",
    "with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(yesterday1), 'w') as f:\n",
    "    url = 'http://news.ltn.com.tw/newspaper/focus/{}'.format(yesterday1)\n",
    "    res = requests.get(url)\n",
    "    soup = bs(res.text)\n",
    "\n",
    "    Times = 'http://news.ltn.com.tw'\n",
    "    sort = soup.select('.newspaper a')\n",
    "\n",
    "    for a in xrange(0, len(sort)):\n",
    "        url2 = Times + sort[a]['href'] #分類網址\n",
    "        res2 = requests.get(url2)\n",
    "        soup2 = bs(res2.text)\n",
    "        #print len(soup2.select('.p_num'))\n",
    "        if len(soup2.select('.p_num')) == 1 : #判斷此類別新聞是否一頁\n",
    "            title = soup2.select('.picword')\n",
    "            title_num = len(soup2.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "            for i in xrange(0 , title_num):   \n",
    "                url1=Times+title[i]['href']   #文章網址\n",
    "                f.write(url1  + '\\n')  #寫入text\n",
    "        else:\n",
    "            for page1 in xrange (1,len(soup2.select('.p_num'))+1): #此類別新聞超過一頁時執行\n",
    "                url3= url2+'?page={}'.format(page1)\n",
    "                #print url3\n",
    "                res3 = requests.get(url3)\n",
    "                soup3 = bs(res3.text)\n",
    "                title = soup3.select('.picword')\n",
    "                title_num1 = len(soup3.select('.picword')) #計算有幾篇文章(每天新聞數量不同)\n",
    "                for i in xrange(0 , title_num1):   \n",
    "                    url1=Times+title[i]['href']   #文章網址\n",
    "                    f.write(url1  + '\\n')  #寫入text\n",
    "\n",
    "\n",
    "# 標題,類別,內文,關鍵詞,新聞連結網址,日期(格式yyyyMMdd)\n",
    "# 爬回\n",
    "\n",
    "def wr_all(url):  #方法 大部分的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')[0].text.encode('utf-8')\n",
    "    date1 =  soup.select('#newstext span ')[0].text.encode('utf-8')\n",
    "    category = soup.select('.guide  a ')[1].text.encode('utf-8')\n",
    "    comp = \" LibertyTimes\"\n",
    "    date = string.replace(date1, '-', '')[0:8]\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text.encode('utf-8')       #加入內文\n",
    "\n",
    "    return tital,date,category, page,comp\n",
    "        \n",
    "def wr_sport(url):  #方法 運動類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)    \n",
    "    essay = soup.select('h4 , p')\n",
    "    tital = soup.select('.Btitle ')[0].text.encode('utf-8')\n",
    "    date1 =  soup.select('.news_content .date ')[0].text.encode('utf-8')\n",
    "    category = \"體育\"\n",
    "    comp = \" LibertyTimes\"\n",
    "    date = string.replace(date1, '/', '')[0:8]\n",
    "\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text.encode('utf-8')       #加入內文  \n",
    "        \n",
    "    return tital,date,category, page,comp\n",
    "\n",
    "def wr_ent(url):  #方法 娛樂類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('p')\n",
    "    tital = soup.select('.news_content h1 ')[0].text.encode('utf-8')\n",
    "    date1 =  soup.select('.news_content .date')[0].text.encode('utf-8')\n",
    "    date = string.replace(date1, '/', '')[0:8]\n",
    "    category = \"娛樂\"\n",
    "    comp = \" LibertyTimes\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text.encode('utf-8')       #加入內文   \n",
    "    return tital,date,category, page,comp\n",
    "\n",
    "def wr_local(url): #方法 地方類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('#newstext p , h4')\n",
    "    tital = soup.select('h1 ')[0].text.encode('utf-8')\n",
    "    date1 =  soup.select('#newstext span ')[0].text.encode('utf-8')\n",
    "    category = \"地方\"\n",
    "    comp = \" LibertyTimes\"\n",
    "    date = string.replace(date1, '-', '')[0:8]\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text.encode('utf-8')       #加入內文\n",
    "    return tital,date,category, page,comp\n",
    "    \n",
    "def wr_talk(url): # 方法 言論類的網頁版型\n",
    "    res = requests.get(url)\n",
    "    res.encoding = \"UTF-8\"\n",
    "    soup = bs(res.text)\n",
    "    essay = soup.select('.cont p')\n",
    "    tital = soup.select('h2 ')[0].text.encode('utf-8')\n",
    "    date1 =  soup.select('.writer span')[0].text.encode('utf-8')\n",
    "    date = string.replace(date1, '-', '')[0:8]\n",
    "    category = \"言論\"\n",
    "    comp = \" LibertyTimes\"\n",
    "    page=\"\"                          #給一個字串,存放內文用           \n",
    "    for div3 in essay[0:(len(essay))]:\n",
    "        page += div3.text.encode('utf-8')       #加入內文    \n",
    "    return tital,date,category, page,comp\n",
    "\n",
    " \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "all_json=\"\"\n",
    "with open('C:/Users/BIG DATA/Desktop/python/essay/{}.txt'.format(yesterday1), 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip()\n",
    "        m = re.match('.*news/(\\w+)/paper/(\\w+)' ,url)  #正規表達法 切出m.group(1)類別 與 m.group(2)網頁最後的數字(用來當檔名)\n",
    "        print m.group(1)\n",
    "        if m.group(1) == 'sports': #判斷是否為體育類\n",
    "            x=  wr_sport(url) #使用方法\n",
    "            data ={'title' :x[0],'category' : x[2],'content' : x[3],'url':url,'date':x[1],'comp':x[4],'keyw':'','hitcount':0,'date2':yesterday1[0:6]}\n",
    "            collection.insert_one(data)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        elif m.group(1) == 'entertainment':#判斷是否為娛樂類\n",
    "            x= wr_ent(url) #使用方法\n",
    "            data ={'title' :x[0],'category' : x[2],'content' : x[3],'url':url,'date':x[1],'comp':x[4],'keyw':'','hitcount':0,'date2':yesterday1[0:6]}\n",
    "            collection.insert_one(data)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "        elif m.group(1) =='local': #判斷是否為地方類\n",
    "            x= wr_local(url) #使用方法\n",
    "            data ={'title' :x[0],'category' : x[2],'content' : x[3],'url':url,'date':x[1],'comp':x[4],'keyw':'','hitcount':0,'date2':yesterday1[0:6]}\n",
    "            collection.insert_one(data)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        elif m.group(1) =='opinion': #判斷是否為言論類\n",
    "            x= wr_talk(url) #使用方法\n",
    "            data ={'title' :x[0],'category' : x[2],'content' : x[3],'url':url,'date':x[1],'comp':x[4],'keyw':'','hitcount':0,'date2':yesterday1[0:6]}\n",
    "            collection.insert_one(data)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "        else:\n",
    "            x= wr_all(url) #使用方法\n",
    "            data ={'title' :x[0],'category' : x[2],'content' : x[3],'url':url,'date':x[1],'comp':x[4],'keyw':'','hitcount':0,'date2':yesterday1[0:6]}\n",
    "            collection.insert_one(data)\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "data.clear()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
