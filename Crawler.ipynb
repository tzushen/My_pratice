{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 1111人力銀行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#抓個縣市每個工作連結\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "cou_url='http://www.1111.com.tw/job-bank/category.asp?cat=1&d=140202&city=1'\n",
    "cou_res = requests.get(cou_url)\n",
    "cou_soup = bs(cou_res.text)\n",
    "cou_divs = cou_soup.select('.step2')[0]\n",
    "all_job= \"\"\n",
    "page_number=0\n",
    "for p in xrange(0,15):\n",
    "    divs1 = cou_divs.select('a')[p]['href']\n",
    "    url1111 = 'http://www.1111.com.tw'\n",
    "    url2222 = url1111+divs1\n",
    "    job_url = url2222+'&page=1'\n",
    "    job_res = requests.get(job_url)\n",
    "    job_soup = bs(job_res.text)  \n",
    "    \n",
    "    page_div = job_soup.select('#showTotalPageCssBottom')[0]\n",
    "    page_div1 = page_div.text.split('/'+' ')[1].split(' ')[0]\n",
    "    page_number = int(page_div1)\n",
    "    if (page_number)>1: \n",
    "        for i in xrange(0,page_number):\n",
    "            job_url1 = url2222+'&page={}'.format(i+1)\n",
    "            job_res1 = requests.get(job_url1)\n",
    "            job_soup1 = bs(job_res1.text)\n",
    "            len_div = len(job_soup1.select('.positionCssA'))\n",
    "            for pages in xrange(0,len_div):\n",
    "                job_href1 = job_soup1.select('.positionCssA')[pages]['href']\n",
    "                all_job += job_href1+'\\n'\n",
    "    else:\n",
    "        job_href = job_soup.select('.positionCssA')[0]['href'] \n",
    "        all_job += job_href+'\\n'\n",
    "    f = open('E:\\job_url\\job.txt','w')\n",
    "    f.write(all_job.encode('utf8'))\n",
    "    f.close()\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#進入每個網頁後爬取裡面內容\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "with open('E:\\job_url\\job.txt', 'r') as f:\n",
    "    for url in f.readlines():\n",
    "        url_content = url\n",
    "        res_content = requests.get(url_content)\n",
    "        soup_content = bs(res_content.text)\n",
    "        divs_content = soup_content.select('#midblock dt')\n",
    "        divs1_content = soup_content.select('#midblock dd')\n",
    "        title = soup_content.select('#commonTop h1')[0] \n",
    "        paper = title.text+' '\n",
    "        len_div = len(divs_content)\n",
    "        for n in range(0,len_div):\n",
    "            x = divs_content[n].text\n",
    "            y = divs1_content[n].text    \n",
    "            dic = {x:y}\n",
    "            for ele in dic:\n",
    "                paper += ele+dic[ele]\n",
    "            name = url_content.split('No=')[1].split('&')[0]         \n",
    "            f = open('E:\\job\\{}.txt'.format(name),'w')\n",
    "            f.write(paper.encode('utf8'))\n",
    "            f.close()\n",
    "            time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 104人力銀行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#查尋軟體相關 一共有15xxx筆 \n",
    "# 問題:單次查詢最多找到3000筆，可能爬取時須細分更多類別爬取\n",
    "\n",
    "#將網址存到文件裡\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "with open('104.txt', 'w') as f:\n",
    "    index = 150\n",
    "    for page in xrange(1, 150):\n",
    "        res = requests.get('http://www.104.com.tw/jobbank/joblist/joblist.cfm?jobsource=104_bank1&ro=0&jobcat=2007001000&order=2&asc=0&page={}&psl=N_B'.format(page))\n",
    "        response = res.text.encode('utf8') \n",
    "        soup = bs(response)\n",
    "        domain = 'http://www.104.com.tw'\n",
    "        print_area = soup.select('.job_name')\n",
    "        # print print_area\n",
    "        for tr in print_area:\n",
    "            if tr.select('a')[0]['href'][0:4] != \"http\":\n",
    "                f.write(domain + tr.select('a')[0]['href']  + '\\n')\n",
    "            else:\n",
    "                continue\n",
    "        time.sleep(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#從文件中抓出連結，爬取網頁資料\n",
    "\n",
    "import re\n",
    "import time\n",
    "dic = {}\n",
    "with open('104.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        url = line.strip()\n",
    "        m = re.match('.*jobno=(\\w+)', url)\n",
    "        try:  #抓出失效連結\n",
    "            jobno = m.group(1)\n",
    "        except:\n",
    "            print url\n",
    "            continue\n",
    "        with open('gov/{}.txt'.format(jobno), 'w') as f:\n",
    "            res = requests.get(url)\n",
    "            soup = bs(res.text)\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                f.write(soup.select('#job , #job h1')[0].prettify('utf-8'))\n",
    "            except:\n",
    "                print url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# cheers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法一:進到網頁判斷有沒有下一頁，如果有則進行下一頁，如果沒有則抓當頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "url=\"http://www.cheers.com.tw/top\"\n",
    "res = requests.get(url)#連到TOP20網頁\n",
    "soup = bs(res.text)#整理內容\n",
    "divs = soup.select('.top20group')[0]#抓到TOP20標籤\n",
    "y = len(divs.select('a'))#以a標籤算長度\n",
    "x_url=\"\"\n",
    "for i in range(0,y):#寫個迴圈把20篇放到x_url\n",
    "    all_page=\"\"#all_page做完回圈後會變空字串\n",
    "    x_url = divs.select('a')[i]['href']#連到每個文章內\n",
    "    res1 = requests.get(x_url)#跟每個頁面取得連結\n",
    "    soup1 = bs(res1.text)#整理內容    \n",
    "    rec_number = soup1.select('.pages')#抓頁數pages標籤\n",
    "    pages_number = rec_number[0]#把陣列裡的第一個東西放到變數裡\n",
    "    i_len = pages_number.select(\"i\")#用i算長度\n",
    "    pages = len(pages_number.select('li'))#用li算長度\n",
    "    pages = pages-2#算頁數(減掉下一頁跟不分頁閱讀)\n",
    "    if (len(i_len)>0):#判斷頁數是否超過1\n",
    "        for i in range(0,pages):#抓取1-20頁\n",
    "            next_page = x_url.split('&')[0]+'&page={}'.format(i+1)#截取&前面字串加上字串&page={}用format方法網頁頁數+1\n",
    "            #url=\"http://www.cheers.com.tw/article/article.action?id=555555&page=i\"\n",
    "            res2 = requests.get(next_page)\n",
    "            soup2 = bs(res2.text)\n",
    "            divs2 = soup2.select('.chw_article_main_content p')\n",
    "            for div3 in divs2[1:(len(divs1)-1)]:#頭尾標題不要\n",
    "                all_page += div3.text #p標籤字串相加存到all_page\n",
    "    else:#抓取當頁\n",
    "        res3 = requests.get(x_url)\n",
    "        soup3 = bs(res3.text)\n",
    "        divs3 = soup3.select('.chw_article_main_content p')\n",
    "        for div4 in divs3[1:(len(divs1)-1)]:\n",
    "            all_page += div4.text #p標籤字串相加\n",
    "    name = x_url.split('id=')[1].split('&')[0]        \n",
    "    f = open('E:\\cheers\\{}.txt'.format(name),'w')\n",
    "    f.write(all_page.encode('utf-8'))\n",
    "    f.close()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 方法二:進到網頁後直接抓取當頁，在判斷有沒有下頁，如果有則進行下一頁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import time\n",
    "url=\"http://www.cheers.com.tw/top\"\n",
    "res = requests.get(url)#連到TOP20網頁\n",
    "soup = bs(res.text)#整理內容\n",
    "divs = soup.select('.top20group')[0]#抓到TOP20標籤\n",
    "y = len(divs.select('a'))#以a標籤算長度\n",
    "x_url=\"\"\n",
    "for i in range(0,y):#寫個迴圈把20篇放到x_url\n",
    "    all_page=\"\"\n",
    "    x_url = divs.select('a')[i]['href']#連到每個文章內\n",
    "    res1 = requests.get(x_url)#跟每個頁面取得連結\n",
    "    soup1 = bs(res1.text)#整理內容    \n",
    "    divs1 = soup1.select('.chw_article_main_content p')\n",
    "    for div in divs1[1:(len(divs1)-1)]:\n",
    "            all_page += div.text #p標籤字串相加\n",
    "    if (len(i_len)>0):\n",
    "        for i in range(0,pages):\n",
    "            next_page = x_url.split('&')[0]+'&page={}'.format(i+1)#截取&前面字串加上字串&page={}用format方法網頁頁數+1\n",
    "            #url=\"http://www.cheers.com.tw/article/article.action?id=555555&page=i\"\n",
    "            res2 = requests.get(next_page)\n",
    "            soup2 = bs(res2.text)\n",
    "            divs2 = soup2.select('.chw_article_main_content p')\n",
    "            for div3 in divs2[1:(len(divs1)-1)]:\n",
    "                all_page += div3.text #p標籤字串相加存到all_page\n",
    "    else:\n",
    "        continue\n",
    "    name = x_url.split('id=')[1].split('&')[0]        \n",
    "    f = open('E:\\cheers\\{}.txt'.format(name),'w')\n",
    "    f.write(all_page.encode('utf-8'))\n",
    "    f.close()\n",
    "    time.sleep(3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
